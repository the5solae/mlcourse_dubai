{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Open Data Science course. Session № 2\n",
    "</center>\n",
    "Authors: Olga Daikhovskaya, Yury Kashnitsky. The material is distributed under [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). You are free to use the material for any non-commercial purposes, mentioning the original authors is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Homework № 7.\n",
    "## <center> Unupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will look at how data dimensionality reduction and clustering methods work. At the same time, we'll practice solving classification task again.\n",
    "\n",
    "We will work with the [Samsung Human Activity Recognition] (https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. Download the data from there. The data comes from the accelerometers and gyros of Samsung Galaxy S3 mobile phones ( you can find more info about the features using on the link above), the type of activity of a person with a phone in his pocket is also known - whether he walked, stood, lay, sat or walked up or down the stairs.\n",
    "\n",
    "First we imagine that the type of activity is unknown to us, and we will try to cluster people purely on the basis of available features. Then we solve the problem of determining the type of physical activity precisely as a classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(['seaborn-darkgrid'])\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "RANDOM_STATE = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"HAPT Data Set/Train/X_train.txt\")\n",
    "y_train = np.loadtxt(\"HAPT Data Set/Train/y_train.txt\").astype(int)\n",
    "\n",
    "X_test = np.loadtxt(\"HAPT Data Set/Test/X_test.txt\")\n",
    "y_test = np.loadtxt(\"HAPT Data Set/Test/y_test.txt\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clustering, we do not need a target vector, so we'll work with the combination of training and test samples. Merge * X_train * with * X_test *, and * y_train * with * y_test *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of unique values of the labels of the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_classes = np.unique(y).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[These labels correspond to:](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names)\n",
    "- 1 WALKING           \n",
    "- 2 WALKING_UPSTAIRS  \n",
    "- 3 WALKING_DOWNSTAIRS\n",
    "- 4 SITTING           \n",
    "- 5 STANDING          \n",
    "- 6 LAYING            \n",
    "- 7 STAND_TO_SIT      \n",
    "- 8 SIT_TO_STAND      \n",
    "- 9 SIT_TO_LIE        \n",
    "- 10 LIE_TO_SIT        \n",
    "- 11 STAND_TO_LIE      \n",
    "- 12 LIE_TO_STAND  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the sample using `StandardScaler` with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "scaler = \n",
    "X_scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of dimensions using PCA, leaving as many components as necessary to explain at least 90% of the variance of the original (scaled) data. Use the scaled sample and fix the random_state (RANDOM_STATE constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "pca = \n",
    "X_pca ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1: ** <br>\n",
    "What is the minimum number of main components required to cover the 90% of the variance of the original (scaled) data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer options:**\n",
    "- 56 \n",
    "- 65\n",
    "- 66\n",
    "- 193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 2:**<br>\n",
    "What percentage of the variance is covered by the first main component? Round to the nearest percent.\n",
    "\n",
    "**Answer options:**\n",
    "- 45\n",
    "- 51\n",
    "- 56\n",
    "- 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data in the projection on the first two main components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "plt.scatter( , , c=y, s=20, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**<br>\n",
    "If everything worked out correctly, you will see a number of clusters, almost perfectly separated from each other. What types of activity are included in these clusters? <br>\n",
    "\n",
    "**Answer options:**\n",
    "- 1 cluster: all 12 activities\n",
    "- 2 clusters: \n",
    "- 3 clusters: \n",
    "- 5 clusters\n",
    "- 12 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the data clustering using the `KMeans` method, training the model on data with a reduced PCA dimension. In this case, we will give a clue to look for exactly 6 clusters, but in general case we will not know how many clusters we should be searching.\n",
    "\n",
    "Options:\n",
    "\n",
    "- ** n_clusters ** = n_classes (number of unique labels of the target class)\n",
    "- ** n_init ** = 100\n",
    "- ** random_state ** = RANDOM_STATE (for the reproducibility of the result)\n",
    "\n",
    "Other parameters should have default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data in the projection on the first two main components. Color the dots according to the clusters received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "plt.scatter( , , c=cluster_labels, s=20, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correspondence between the cluster marks and the original class labels and what kinds of activities the `KMeans` algorithm is confused at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(y, cluster_labels, margins=True)\n",
    "tab.index = ['WALKING', 'WALKING_UPSTAIRS',\n",
    "            'WALKING_DOWNSTAIRS', 'SITTING', 'LAYING', 'STAND_TO_SIT','SIT_TO_STAND', 'SIT_TO_STAND', 'SIT_TO_LIE',' LIE_TO_SIT',\n",
    "             'STAND_TO_LIE','LIE_TO_STAND','all']\n",
    "tab.columns = ['cluster' + str(i + 1) for i in range(12)] + ['all']\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each class (i.e., each activity) there are several clusters. Let's look at the maximum percentage of objects in a class that are assigned to a single cluster. This will be a simple metric that characterizes how easily the class separates from others when clustering.\n",
    "\n",
    "Example: if for the class of \"WALKING_UPSTAIRS\", in which there are 1544 objects, the distribution of clusters is:\n",
    " - cluster 1 - 0\n",
    " - cluster 2 - 0\n",
    " - cluster 3 - 949\n",
    " -  ....\n",
    " - cluster 12 - 357\n",
    " \n",
    " then such a share will be 949/1544 $ \\ approx $ 0.61.\n",
    "\n",
    "** Question 4: ** <br>\n",
    "Which activity has been separated from the rest better than others based on the simple metric described above? <br>\n",
    "\n",
    "**Answer:**\n",
    "- 1 WALKING           \n",
    "- 2 WALKING_UPSTAIRS  \n",
    "- 3 WALKING_DOWNSTAIRS\n",
    "- 4 SITTING           \n",
    "- 5 STANDING          \n",
    "- 6 LAYING            \n",
    "- 7 STAND_TO_SIT      \n",
    "- 8 SIT_TO_STAND      \n",
    "- 9 SIT_TO_LIE        \n",
    "- 10 LIE_TO_SIT        \n",
    "- 11 STAND_TO_LIE      \n",
    "- 12 LIE_TO_STAND  \n",
    "- there is no correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the elbow method to select the optimal number of clusters. Parameters of the algorithm and the data we use are the same as before, we change only `n_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc1386de6b943aaa93c1af82470d3e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "inertia = []\n",
    "for k in tqdm_notebook(range(1, n_classes + 1)):\n",
    "    #\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We calculate $ D(k) $, as described in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 5: ** <br>\n",
    "How many clusters can we choose according to the elbow method? <br>\n",
    "\n",
    "**Answer options:**\n",
    "- 1\n",
    "- 2\n",
    "- 3\n",
    "- 4\n",
    "- 5\n",
    "- 6\n",
    "- 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another method of clustering, which was described in the article - agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ag = AgglomerativeClustering(n_clusters=n_classes, \n",
    "                             linkage='ward').fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Adjusted Rand Index (`sklearn.metrics`) for the resulting clustering and for ` KMeans` with the parameters from the 4th question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 6: ** <br>\n",
    "Select all the correct statements. <br>\n",
    "\n",
    "** Answer options: **\n",
    "- ARI expresses the similarity of the tags obtained after clustering, with class labels for the same sample and the higher the value of this index, the better\n",
    "- According to ARI, KMeans handled clustering worse than Agglomerative Clustering\n",
    "- For ARI, it does not matter which tags are assigned to the cluster, only the partitioning of objects into clusters is important\n",
    "- In case of random partitioning into clusters ARI will be close to zero\n",
    "\n",
    "\n",
    "\n",
    "**Comment:**\n",
    "Check ARI documentation in sklearn docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the task is not very well solved just as a clustering task if you select several clusters (> 2). Now, let's solve the classification problem, given that the data is marked up.\n",
    "\n",
    "For classification, use the support vector machine - class `sklearn.svm.LinearSVC`. in this course, we did not review on this algorithm separately, but it is well-known and you can read about it, for example, in the materials of Yevgeny Sokolov - [here](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem11_linear.pdf).\n",
    "\n",
    "Choose the `C` hyperparameter for` LinearSVC` using `GridSearchCV`.\n",
    "\n",
    "- Train the new `StandardScaler` on the training sample (with all the original features), apply scaling to the test sample\n",
    "- In `GridSearchCV`, specify cv = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#\n",
    "X_train_scaled = \n",
    "X_test_scaled = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=RANDOM_STATE)\n",
    "svc_params = {'C': [0.001, 0.01, 0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "best_svc = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc.best_params_, best_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**<br>\n",
    "Which value of the hyperparameter `C` was chosen the best on the basis of cross-validation? <br>\n",
    "\n",
    "**Answer options:**\n",
    "- 0.001\n",
    "- 0.01\n",
    "- 0.1,\n",
    "- 1\n",
    "- 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = best_svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.crosstab(y_test, y_predicted, margins=True)\n",
    "tab.index = ['WALKING', 'WALKING_UPSTAIRS',\n",
    "            'WALKING_DOWNSTAIRS', 'SITTING', 'LAYING', 'STAND_TO_SIT','SIT_TO_STAND', 'SIT_TO_STAND', 'SIT_TO_LIE',' LIE_TO_SIT',\n",
    "             'STAND_TO_LIE','LIE_TO_STAND','all']\n",
    "tab.columns = ['WALKING', 'WALKING_UPSTAIRS',\n",
    "            'WALKING_DOWNSTAIRS', 'SITTING', 'LAYING', 'STAND_TO_SIT','SIT_TO_STAND', 'SIT_TO_STAND', 'SIT_TO_LIE',' LIE_TO_SIT',\n",
    "             'STAND_TO_LIE','LIE_TO_STAND','all']\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classification problem is solved quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 8: ** <br>\n",
    "Does SVM confuse the classes inside the activity groups we identified earlier (in the question 3)? We consider that it is confused, if the algorithm was wrong at least in one case. <br>\n",
    "\n",
    "**Answer options:**\n",
    "- yes\n",
    "- no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do the same thing as in Question 7, but add the PCA.\n",
    "\n",
    "- Use the samples `X_train_scaled` and` X_test_scaled`\n",
    "- Teach the same PCA as before, on a scaled training sample, apply the conversion to a test\n",
    "- Choose the hyperparameter `C` with cross-validation on the training sample with PCA-transformation. You will notice how much faster it works than before.\n",
    "\n",
    "** Question 9: ** <br>\n",
    "What is the difference between the best quality (the proportion of correct answers) for cross-validation in the case of all 561 initial characteristics and in the second case, when the principal component method was applied? Round to the nearest percent. <br>\n",
    "\n",
    "** Options: **\n",
    "- The quality is the same\n",
    "- 2%\n",
    "- 4%\n",
    "- 10%\n",
    "- 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 10: ** <br>\n",
    "Select all the correct statements:\n",
    "\n",
    "** Answer options: **\n",
    "- Principal component analysis in this case allowed to reduce the model training time, while the quality (the proportion of correct responses on cross-validation) suffered greatly, by more than 10%\n",
    "- PCA can be used to visualize data, but there are better methods for this task, for example, tSNE. But PCA has less computational complexity\n",
    "- PCA builds linear combinations of initial characteristics that are poorly interpreted by humans\n",
    "- SVM works better than kMeans, since it clearly reduces the algorithm to the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
