{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DmitriiDenisov/mlcourse_dubai/blob/master/jupyter_notebooks/Lecture_04_Linear_Regression/why_punish_large_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zUX6kgo2Gfm"
   },
   "source": [
    "Let's assume that we have multicollinearity case. \n",
    "\n",
    "In our toy example feature_2 = 2 * feature_1\n",
    "\n",
    "feature_0 is constant feaute that is constant 1 (we talked about it in the begging of the lecture, do you rememeber it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Fgh0nGC2Gfo",
    "outputId": "f0cff6e9-6de7-445c-92e4-217fc3178fff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2\n",
       "0        1.0          0          0\n",
       "1        1.0          1          2\n",
       "2        1.0          2          4\n",
       "3        1.0          3          6\n",
       "4        1.0          4          8\n",
       "5        1.0          5         10\n",
       "6        1.0          6         12\n",
       "7        1.0          7         14\n",
       "8        1.0          8         16\n",
       "9        1.0          9         18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X = pd.DataFrame({'feature_0': np.ones(10), 'feature_1': np.arange(10), 'feature_2': 2*np.arange(10)})\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glNL6eJs2Gfs"
   },
   "source": [
    "Let's also assume that we found a solution and it is $w_0 = -3, w_1 = 3, w_2 = 4 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw4HHneT2Gfs"
   },
   "outputs": [],
   "source": [
    "w = np.array([-3, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfYRB8hW2Gfw"
   },
   "source": [
    "Fine, now let's calculate the prdiction of our model. I will remind that our prediction is calulaed as $\\hat{y} = X \\hat{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FlVNAoy2Gfw",
    "outputId": "41512c10-021b-4e8d-c706-641bbbc4d639"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  8., 19., 30., 41., 52., 63., 74., 85., 96.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Obd9NT8r2Gf0"
   },
   "source": [
    "Okay, we got our prediction. Now let's some other values of $w_1=10, w_2=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7H4ICdqI2Gf0",
    "outputId": "40ca9a59-3509-4d98-a270-022b825d8109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  8., 19., 30., 41., 52., 63., 74., 85., 96.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([-3, 10, 0.5])\n",
    "np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZsK4V1Z2Gf3"
   },
   "source": [
    "We got exactly same prediction, didn't we? That literally means that we got two identical models with different parameters. In terms of optimization task these models are euqal because they give exactly the same result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q1yoAjb2Gf4"
   },
   "source": [
    "Now let's consider even more crazy example which will give us exactly same silution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Srb-oTZP2Gf4",
    "outputId": "25469ecb-2402-463e-c9ba-ec880e7dbc5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  8., 19., 30., 41., 52., 63., 74., 85., 96.])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([-3, 1234123421., -617061705])\n",
    "np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5qmKDTu2Gf7"
   },
   "source": [
    "And more crazy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OfzErKp2Gf8",
    "outputId": "e8191e2e-6cfc-485d-bfdb-5e238500fb7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  8., 19., 30., 41., 52., 63., 74., 85., 96.])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([-3, -1243234123421., 621617061716])\n",
    "np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSLBRmyE2Gf_"
   },
   "source": [
    "As you probably already guessed in case of multicollinearity we have inifinite number of models which give same result, i.e. while solving an optimization task we can end up with any of those models and not all of them have good generalizing ability, because of large coefficients is very sensitive to very small changes in features and consequently our model is overfitted. Then Lasso or Ridge come to the game"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "why_punish_large_weights.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
